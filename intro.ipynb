{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd728b94",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß† Introduction to Apache Airflow \n",
    "\n",
    "### üü© 1. Introduction\n",
    "Apache Airflow is a platform designed to **create, schedule, and monitor workflows** programmatically. It is widely used in data engineering for orchestrating complex workflows and automating data pipelines.\n",
    "\n",
    "Instructor: **Mike Metzger**, a Data Engineer.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 2. Understanding Data Engineering\n",
    "\n",
    "- **Data Engineering** involves transforming raw data into a format that is reliable, repeatable, and maintainable.\n",
    "- It includes designing systems that move, clean, and organize data.\n",
    "- The goal is to **automate and optimize** all processes involving data handling and transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 3. What is a Workflow?\n",
    "\n",
    "- A **workflow** is a series of steps to complete a specific data task.\n",
    "- These steps can include:\n",
    "  - Downloading files\n",
    "  - Copying or moving data\n",
    "  - Filtering or cleaning data\n",
    "  - Writing data to a database\n",
    "- Workflows can be simple (2‚Äì3 steps) or very complex (hundreds of steps).\n",
    "- In data engineering, a workflow describes the **flow of data** and the sequence of operations applied to it.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 4. What is Apache Airflow?\n",
    "\n",
    "- **Apache Airflow** is an open-source platform that allows users to:\n",
    "  - Program workflows using **Python**\n",
    "  - **Schedule** them to run automatically\n",
    "  - **Monitor** them through a web interface, CLI, or REST API\n",
    "- Airflow enables visibility and control over data processes, ensuring they are **maintainable and observable**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 5. Core Concept: DAG (Directed Acyclic Graph)\n",
    "\n",
    "- A **DAG** is the fundamental structure used in Airflow to define a workflow.\n",
    "- In simple terms:\n",
    "  - **Directed**: The tasks follow a defined direction (one task flows into another).\n",
    "  - **Acyclic**: There are no circular dependencies.\n",
    "- A DAG is a **graph** where:\n",
    "  - **Nodes = Tasks**\n",
    "  - **Edges = Dependencies between tasks**\n",
    "\n",
    "#### DAG Components:\n",
    "- `dag_id`: Unique identifier for the DAG\n",
    "- `start_date`: When the DAG is allowed to start running\n",
    "- `default_args`: Dictionary containing settings like retries, email alerts, etc.\n",
    "- Owner and email notification configurations can also be included\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from airflow import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2024, 1, 10),\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_pipeline', default_args=default_args)\n",
    "```\n",
    "\n",
    "> In the shell (CLI), you refer to the DAG using `dag_id` (e.g., `etl_pipeline`), not the variable name.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 6. Running Workflows (Tasks) in Airflow\n",
    "\n",
    "- A **task** is a single unit of work in a DAG.\n",
    "- Tasks can be tested or run using the CLI.\n",
    "\n",
    "#### Command:\n",
    "```bash\n",
    "airflow tasks test <dag_id> <task_id> <execution_date>\n",
    "```\n",
    "\n",
    "#### Example:\n",
    "```bash\n",
    "airflow tasks test example-etl download-file 2024-01-10\n",
    "```\n",
    "\n",
    "- This command manually triggers the `download-file` task inside the DAG `example-etl` for January 10, 2024.\n",
    "- It is useful for testing individual components of a DAG before scheduling the whole pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 7. Airflow Interfaces\n",
    "\n",
    "Airflow can be interacted with in several ways:\n",
    "\n",
    "- **Code (Python)** ‚Äì to define and configure DAGs and tasks.\n",
    "- **Command-Line Interface (CLI)** ‚Äì to trigger tasks and manage workflows manually.\n",
    "- **Web UI** ‚Äì to visualize DAGs, monitor runs, logs, and status of tasks.\n",
    "- **REST API** ‚Äì to programmatically manage DAGs and tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© 8. Comparison with Other Tools\n",
    "\n",
    "While Airflow is powerful, it‚Äôs not the only tool for managing workflows:\n",
    "\n",
    "| Tool         | Description                                  |\n",
    "|--------------|----------------------------------------------|\n",
    "| **Luigi**    | Created by Spotify, focuses on dependency resolution. |\n",
    "| **SSIS**     | Microsoft SQL Server Integration Services, for ETL in MS environments. |\n",
    "| **Bash Scripting** | Lightweight automation; used for simple tasks. |\n",
    "\n",
    "> Airflow integrates well with Bash, and scripts can be part of DAG tasks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858703f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üî∑ Airflow DAGs\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 1. Introduction to DAGs in Airflow\n",
    "\n",
    "- DAG stands for **Directed Acyclic Graph**.\n",
    "- DAGs are the **primary building blocks** in Airflow workflows.\n",
    "- They define the **order and dependency** of tasks in a workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 2. What is a DAG?\n",
    "\n",
    "A DAG has three important properties:\n",
    "\n",
    "1. **Directed**:  \n",
    "   - Tasks have a defined order of execution.  \n",
    "   - The direction ensures dependencies are followed properly (e.g., Task A runs before Task B).\n",
    "\n",
    "2. **Acyclic**:  \n",
    "   - There are **no loops**; tasks do not repeat within the same run.  \n",
    "   - However, the **entire DAG** can be rerun.\n",
    "\n",
    "3. **Graph**:  \n",
    "   - The structure represents tasks (nodes) and dependencies (edges).  \n",
    "   - It provides a **clear and logical representation** of a workflow.\n",
    "\n",
    "> üß† Note: The DAG concept is common in data engineering tools like **Apache Spark**, **dbt**, and others‚Äînot just in Airflow.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 3. DAGs in Airflow\n",
    "\n",
    "- In Airflow, DAGs are defined using **Python code**.\n",
    "- The DAG describes how tasks are related and when they should be executed.\n",
    "- Although DAGs are defined in Python, tasks themselves can execute:\n",
    "  - **Python functions**\n",
    "  - **Bash scripts**\n",
    "  - **Spark jobs**\n",
    "  - **Custom operators**\n",
    "- Airflow breaks DAGs into **tasks**, which can be:\n",
    "  - **Operators** (pre-built functionality like BashOperator, PythonOperator)\n",
    "  - **Sensors** (wait for external events or states)\n",
    "- Dependencies between tasks can be **explicit** or **implicit**.\n",
    "\n",
    "> Example of a dependency:  \n",
    "> `task_a >> task_b` means task_b depends on task_a.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 4. How to Define a DAG in Python\n",
    "\n",
    "#### Step-by-step DAG setup:\n",
    "\n",
    "1. **Import DAG class**:\n",
    "```python\n",
    "from airflow import DAG\n",
    "```\n",
    "\n",
    "2. **Define default arguments** (optional but recommended):\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'jdoe',\n",
    "    'email': ['jdoe@example.com'],\n",
    "    'start_date': datetime(2024, 1, 1)\n",
    "}\n",
    "```\n",
    "\n",
    "3. **Create DAG object using context manager** (recommended for Airflow 2.x+):\n",
    "```python\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id='etl_workflow',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',  # Optional\n",
    "    catchup=False                # Optional\n",
    ") as etl_dag:\n",
    "    # Task definitions go here\n",
    "    pass\n",
    "```\n",
    "\n",
    "4. **Alternate (pre-Airflow 2.x) method**:\n",
    "```python\n",
    "etl_dag = DAG('etl_workflow', default_args=default_args)\n",
    "# Task definitions would be written under this DAG variable\n",
    "```\n",
    "\n",
    "> ‚úÖ Both methods are functionally valid. The `with` statement is newer and more readable.\n",
    "\n",
    "> ‚ö†Ô∏è `DAG` is case-sensitive in Python. Use it exactly as defined.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 5. DAGs on the Command Line\n",
    "\n",
    "Airflow provides a robust **command-line interface (CLI)** for interacting with DAGs:\n",
    "\n",
    "- General help:\n",
    "  ```bash\n",
    "  airflow -h\n",
    "  ```\n",
    "\n",
    "- List all registered DAGs:\n",
    "  ```bash\n",
    "  airflow dags list\n",
    "  ```\n",
    "\n",
    "- Other useful CLI commands:\n",
    "  - `airflow dags trigger <dag_id>` ‚Äì manually trigger a DAG run\n",
    "  - `airflow dags pause <dag_id>` ‚Äì pause a DAG\n",
    "  - `airflow dags unpause <dag_id>` ‚Äì resume a paused DAG\n",
    "  - `airflow dags show <dag_id>` ‚Äì visualize the DAG structure\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 6. When to Use CLI vs Python\n",
    "\n",
    "| Task                                  | Use CLI                     | Use Python                  |\n",
    "|---------------------------------------|-----------------------------|-----------------------------|\n",
    "| Starting Airflow processes            | ‚úÖ                          | ‚ùå                          |\n",
    "| Triggering DAGs or tasks manually     | ‚úÖ                          | ‚ùå                          |\n",
    "| Viewing logs or task states           | ‚úÖ                          | ‚ùå                          |\n",
    "| Creating DAGs and defining workflows  | ‚ùå                          | ‚úÖ                          |\n",
    "| Writing business logic or data steps  | ‚ùå                          | ‚úÖ                          |\n",
    "\n",
    "> üõ† CLI is operational, Python is developmental.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae602d",
   "metadata": {},
   "source": [
    "### **Defining a simple DAG**\n",
    "You've spent some time reviewing the Airflow components and are interested in testing out your own workflows. To start you decide to define the default arguments and create a DAG object for your workflow.\n",
    "\n",
    "The DateTime object has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DAG object\n",
    "from airflow import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2023, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "with DAG('example_etl', default_args=default_args) as etl_dag:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a918cdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "960c0e3d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üî∑ Airflow Web Interface ‚Äì Detailed Notes\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 1. Introduction to the Airflow Web UI\n",
    "\n",
    "- The **Airflow Web Interface (UI)** is a graphical tool that helps in:\n",
    "  - Monitoring DAGs\n",
    "  - Triggering or pausing workflows\n",
    "  - Viewing task status and logs\n",
    "  - Debugging workflows\n",
    "\n",
    "- The web UI complements the CLI but is often more **intuitive and visual**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 2. DAGs View (Main Page)\n",
    "\n",
    "This is the **landing page** of the Airflow UI. It gives an **overview of all available DAGs**.\n",
    "\n",
    "#### Key components visible in the DAGs View:\n",
    "\n",
    "| Section             | Description |\n",
    "|---------------------|-------------|\n",
    "| **DAG Name**         | A list of all DAGs/workflows present in the system |\n",
    "| **Owner**            | Who created or owns the DAG (from `default_args`) |\n",
    "| **Schedule**         | Cron string or time-based trigger for DAG execution |\n",
    "| **Last Run**         | Timestamp of the most recent DAG execution |\n",
    "| **Next Run**         | Scheduled time for the next DAG run |\n",
    "| **Recent Tasks**     | Small icons or indicators showing status of last few task runs (success, failed, etc.) |\n",
    "| **Actions**          | Options to trigger DAG, pause it, delete it, or view details |\n",
    "\n",
    "> üîó Clicking on a DAG name (e.g., `example_dag`) takes you to its **Detail Page**.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 3. DAG Detail View\n",
    "\n",
    "This view provides **in-depth access to one specific DAG**. It includes several subviews:\n",
    "\n",
    "#### Key components:\n",
    "\n",
    "1. **Grid View** (Default):\n",
    "   - Displays a table/grid of DAG runs and task states.\n",
    "   - Shows task names, operator types, and dependencies visually.\n",
    "   - Useful for checking which tasks succeeded, failed, or are running.\n",
    "\n",
    "2. **Graph View**:\n",
    "   - Presents a **graphical flowchart** of tasks and their dependencies.\n",
    "   - Task nodes display type (e.g., BashOperator).\n",
    "   - Useful to understand the logical structure and flow of the DAG.\n",
    "\n",
    "3. **Code View**:\n",
    "   - Shows the actual **Python code** used to define the DAG.\n",
    "   - This is **read-only** ‚Äì edits must be done in the script file.\n",
    "   - Helps quickly inspect what a DAG does (e.g., `echo $RANDOM` in BashOperator).\n",
    "\n",
    "4. **Other Functional Tabs**:\n",
    "   - **Task Duration**: How long each task took to run.\n",
    "   - **Task Tries**: Number of attempts made for each task.\n",
    "   - **Gantt Chart**: Visual timeline showing when tasks started and finished.\n",
    "   - **Run History**: Past executions of the DAG.\n",
    "\n",
    "#### Actions available:\n",
    "- **Trigger DAG** ‚Äì manually start a DAG run.\n",
    "- **Refresh** ‚Äì update the UI with current info.\n",
    "- **Delete** ‚Äì remove a DAG from the Airflow environment.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 4. Audit Logs (under Browse Menu)\n",
    "\n",
    "- Logs provide **system-level insight and user activity tracking**.\n",
    "- Tracks events such as:\n",
    "  - Starting/stopping the webserver\n",
    "  - Viewing DAG graph/code\n",
    "  - Creating users\n",
    "  - Triggering DAGs\n",
    "- **Event Type** column helps filter logs (e.g., `cli`, `scheduler`, `webserver`, `graph`, etc.)\n",
    "\n",
    "> üîç Frequent log inspection helps understand system behavior and debug problems.\n",
    "\n",
    "---\n",
    "\n",
    "### üü¶ 5. Web UI vs Command Line Tool\n",
    "\n",
    "| Aspect                      | Web UI                              | Command Line Tool (CLI)       |\n",
    "|-----------------------------|-------------------------------------|-------------------------------|\n",
    "| User Experience             | Intuitive, visual                   | Fast, scriptable              |\n",
    "| Setup/Access                | May require browser access          | SSH or local terminal         |\n",
    "| Triggering DAGs             | ‚úÖ Easy to click                     | ‚úÖ Via `airflow dags trigger` |\n",
    "| Viewing DAGs/Logs           | ‚úÖ Visual detail                     | ‚úÖ Text-based output          |\n",
    "| Editing DAG code            | ‚ùå (Read-only)                      | ‚ùå (Requires file editor)     |\n",
    "| Suitable for                | Monitoring and administration       | Advanced users and automation |\n",
    "\n",
    "> üß© Most developers use **both interfaces** based on what they are trying to accomplish.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80db984",
   "metadata": {},
   "source": [
    "**Airflow CLI commands**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Help**\n",
    "```bash\n",
    "airflow -h\n",
    "```\n",
    "üìò *Displays help info and lists all available commands.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **List DAGs**\n",
    "```bash\n",
    "airflow dags list\n",
    "```\n",
    "üîç *Lists all DAGs available in the Airflow environment.*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Trigger a DAG**\n",
    "```bash\n",
    "airflow dags trigger <dag_id>\n",
    "```\n",
    "üöÄ *Manually triggers a DAG run.*\n",
    "\n",
    "- `dag_id`: ID of the DAG to run\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Start Webserver**\n",
    "```bash\n",
    "airflow webserver\n",
    "```\n",
    "üåê *Starts the Airflow web UI (default at `localhost:8080`).*\n",
    "\n",
    "**Common flags:**\n",
    "- `-p <port>`: Change the port (default is 8080)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Start Scheduler**\n",
    "```bash\n",
    "airflow scheduler\n",
    "```\n",
    "üìÖ *Starts the Airflow scheduler to execute scheduled DAGs.*\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Test a Task**\n",
    "```bash\n",
    "airflow tasks test <dag_id> <task_id> <execution_date>\n",
    "```\n",
    "üß™ *Runs a single task instance without dependencies.*\n",
    "\n",
    "- `dag_id`: DAG containing the task  \n",
    "- `task_id`: Task to test  \n",
    "- `execution_date`: Date for task run (`YYYY-MM-DD` format)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **View Task Logs**\n",
    "```bash\n",
    "airflow tasks logs <dag_id> <task_id> <execution_date>\n",
    "```\n",
    "üìÑ *Displays logs for a given task run.*\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Pause a DAG**\n",
    "```bash\n",
    "airflow dags pause <dag_id>\n",
    "```\n",
    "‚è∏Ô∏è *Prevents a DAG from being scheduled automatically.*\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Unpause a DAG**\n",
    "```bash\n",
    "airflow dags unpause <dag_id>\n",
    "```\n",
    "‚ñ∂Ô∏è *Resumes automatic scheduling of a paused DAG.*\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **List Tasks in a DAG**\n",
    "```bash\n",
    "airflow tasks list <dag_id>\n",
    "```\n",
    "üìã *Shows all tasks within a specified DAG.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9cd9f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üîç **How to Examine the DAG**\n",
    "1. **Go to the Airflow Web UI** (typically at `http://localhost:8080`).\n",
    "2. In the **DAGs list**, locate and click on the DAG named `update_state`.\n",
    "3. On the DAG detail page, explore the following views:\n",
    "   - **Graph View**: Shows tasks and their dependencies with operator types.\n",
    "   - **Tree View**: Displays task structure and run statuses over time.\n",
    "   - **Code View**: Lets you view the raw Python code defining the DAG.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Determine Operators in Use**\n",
    "- In the **Graph View**, each task usually shows the operator used (e.g., `BashOperator`, `PythonOperator`, etc.).\n",
    "- Alternatively, in the **Code View**, look for lines like:\n",
    "  ```python\n",
    "  PythonOperator(...)\n",
    "  BashOperator(...)\n",
    "  DummyOperator(...)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì **Question Task**\n",
    "Your task is to identify **which operator is NOT used** in `update_state`.\n",
    "\n",
    "To answer:\n",
    "- Open **Code View**.\n",
    "- Search for operator classes used.\n",
    "- Compare against a list of common operators (e.g., `PythonOperator`, `BashOperator`, `DummyOperator`, `EmailOperator`).\n",
    "\n",
    "> üîé **Example**: If you see `PythonOperator` and `DummyOperator`, but **no reference to `BashOperator`**, then **BashOperator is NOT used**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18fd4c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Airflow Operators Overview**\n",
    "**Operators** in Airflow define **what** each task does. Each operator represents a **single task** in the DAG.\n",
    "\n",
    "- Operators are **self-contained** ‚Äì they typically don‚Äôt share data with other tasks.\n",
    "- Common operator categories:\n",
    "  - Bash-related (e.g., `BashOperator`)\n",
    "  - Python-based (e.g., `PythonOperator`)\n",
    "  - Dummy/Placeholder (e.g., `EmptyOperator`)\n",
    "  - Email notifications, Docker tasks, sensors, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Key Airflow Operators with Description & Arguments**\n",
    "\n",
    "### 1. **BashOperator**\n",
    "Runs any valid Bash command.\n",
    "\n",
    "```python\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "BashOperator(\n",
    "    task_id='print_hello',\n",
    "    bash_command='echo \"Hello World!\"'\n",
    ")\n",
    "```\n",
    "\n",
    "**Common Arguments:**\n",
    "- `task_id`: Unique ID of the task.\n",
    "- `bash_command`: The actual bash command or script.\n",
    "- `env`: (optional) Dictionary of environment variables.\n",
    "- `cwd`: (optional) Working directory to run command in.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **PythonOperator**\n",
    "Executes a Python function.\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "PythonOperator(\n",
    "    task_id='run_my_func',\n",
    "    python_callable=my_function\n",
    ")\n",
    "```\n",
    "\n",
    "**Common Arguments:**\n",
    "- `task_id`\n",
    "- `python_callable`: The function to call.\n",
    "- `op_args`: (optional) Positional args for the function.\n",
    "- `op_kwargs`: (optional) Keyword args for the function.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **EmptyOperator** *(Previously DummyOperator)*\n",
    "Used as a placeholder or for DAG structure/visualization.\n",
    "\n",
    "```python\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "EmptyOperator(\n",
    "    task_id='start_pipeline'\n",
    ")\n",
    "```\n",
    "\n",
    "**Common Arguments:**\n",
    "- Only `task_id` is usually needed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **EmailOperator**\n",
    "Sends an email notification.\n",
    "\n",
    "```python\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "EmailOperator(\n",
    "    task_id='send_email',\n",
    "    to='user@example.com',\n",
    "    subject='Airflow Alert',\n",
    "    html_content='<p>DAG completed!</p>'\n",
    ")\n",
    "```\n",
    "\n",
    "**Common Arguments:**\n",
    "- `to`: Recipient email.\n",
    "- `subject`: Email subject.\n",
    "- `html_content`: Body content.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **Operator Gotchas**\n",
    "- Operators **run in isolated environments**.\n",
    "- Use `env` to pass variables.\n",
    "- Be mindful of **elevated privileges** and restricted directories.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7476315",
   "metadata": {},
   "source": [
    "Here‚Äôs a complete **Airflow DAG** that brings together the operators we discussed ‚Äî `BashOperator`, `PythonOperator`, `EmptyOperator`, and `EmailOperator`. It includes detailed comments to help you understand each component:\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define a simple Python function for PythonOperator\n",
    "def greet():\n",
    "    print(\"Hello from Python!\")\n",
    "\n",
    "# Define default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email': ['alerts@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    dag_id='example_combined_operator_dag',\n",
    "    description='A DAG demonstrating Bash, Python, Empty, and Email operators',\n",
    "    schedule_interval=None,  # manual trigger only\n",
    "    default_args=default_args,\n",
    "    catchup=False,\n",
    "    tags=['example', 'operators']\n",
    ") as dag:\n",
    "\n",
    "    # Start placeholder task (EmptyOperator)\n",
    "    start = EmptyOperator(\n",
    "        task_id='start'\n",
    "    )\n",
    "\n",
    "    # BashOperator task to print a message\n",
    "    bash_task = BashOperator(\n",
    "        task_id='bash_echo',\n",
    "        bash_command='echo \"This is a BashOperator task.\"',\n",
    "        env={'MY_ENV_VAR': '123'}  # Example environment variable\n",
    "    )\n",
    "\n",
    "    # PythonOperator task to run a Python function\n",
    "    python_task = PythonOperator(\n",
    "        task_id='python_greeting',\n",
    "        python_callable=greet\n",
    "    )\n",
    "\n",
    "    # Another BashOperator to simulate a script run\n",
    "    clean_task = BashOperator(\n",
    "        task_id='run_data_cleaning',\n",
    "        bash_command='cat /etc/passwd | awk -F \":\" \\'{print $1}\\''\n",
    "    )\n",
    "\n",
    "    # EmailOperator task to notify completion\n",
    "    notify = EmailOperator(\n",
    "        task_id='send_email',\n",
    "        to='team@example.com',\n",
    "        subject='DAG Completed Successfully',\n",
    "        html_content='<p>The DAG has finished running.</p>'\n",
    "    )\n",
    "\n",
    "    # End placeholder task\n",
    "    end = EmptyOperator(\n",
    "        task_id='end'\n",
    "    )\n",
    "\n",
    "    # Set task dependencies\n",
    "    start >> bash_task >> python_task >> clean_task >> notify >> end\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† What this DAG does:\n",
    "- **`start`**: Begins the pipeline (dummy task).\n",
    "- **`bash_task`**: Echoes a simple message.\n",
    "- **`python_task`**: Executes a Python function (`greet`).\n",
    "- **`clean_task`**: Runs a simple bash data-cleaning simulation.\n",
    "- **`notify`**: Sends an email when done.\n",
    "- **`end`**: Marks the end of the workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab67a1",
   "metadata": {},
   "source": [
    "---\n",
    "### **Airflow Tasks & Dependencies**\n",
    "\n",
    "1. **Airflow Operators**:\n",
    "   - **Definition**: Operators represent individual tasks in a workflow. Common types include `BashOperator`, `PythonOperator`, etc.\n",
    "   - **Command-Line ID**: `airflow operators list` - Lists all available operators.\n",
    "\n",
    "2. **Tasks**:\n",
    "   - **Definition**: Tasks are instantiated operators. They are assigned to variables like `task1` or `task2`.\n",
    "   - **Command-Line ID**: `airflow dags list_tasks <dag_id>` - Lists all tasks in a specific DAG.\n",
    "\n",
    "3. **Task Dependencies**:\n",
    "   - **Definition**: Task dependencies define the order of execution (upstream and downstream). Dependencies can be set using bitshift operators (`>>` for downstream, `<<` for upstream).\n",
    "   - **Command-Line ID**: `airflow tasks list <dag_id>` - Shows the task dependency relationships in a DAG.\n",
    "\n",
    "4. **Upstream vs Downstream**:\n",
    "   - **Definition**: `Upstream` means before (task must complete first), and `Downstream` means after (task runs after another).\n",
    "   - **Command-Line ID**: `airflow tasks clear <dag_id>` - Clears task states, allowing reruns and rescheduling.\n",
    "\n",
    "5. **Simple Task Dependency**:\n",
    "   - **Definition**: Example of using bitshift operators to chain tasks (task1 >> task2 means task1 runs before task2).\n",
    "   - **Command-Line ID**: `airflow dags trigger <dag_id>` - Manually triggers a DAG to run.\n",
    "\n",
    "6. **Task Dependencies in the Airflow UI**:\n",
    "   - **Definition**: In the UI, task dependencies are visually represented with arrows showing which task runs before or after another.\n",
    "   - **Command-Line ID**: `airflow dags list` - Lists all DAGs to view dependencies and runs.\n",
    "\n",
    "7. **Multiple Dependencies**:\n",
    "   - **Definition**: Complex dependencies can be set using bitshift operators in various configurations to define complex workflows.\n",
    "   - **Command-Line ID**: `airflow tasks dependencies <dag_id>` - Displays task dependencies graphically.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7f5c2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Airflow DAG Example: Task Creation + Dependencies**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Default DAG arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "# Define the DAG context\n",
    "with DAG(\n",
    "    dag_id='task_dependency_example_dag',\n",
    "    description='DAG to demonstrate task instantiation and dependencies',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=None,\n",
    "    catchup=False,\n",
    "    tags=['example', 'task-dependencies']\n",
    ") as dag:\n",
    "\n",
    "    # Start task using EmptyOperator (acts as a placeholder)\n",
    "    start = EmptyOperator(\n",
    "        task_id='start'\n",
    "    )\n",
    "\n",
    "    # Task 1: Run bash command\n",
    "    task1 = BashOperator(\n",
    "        task_id='first_task',\n",
    "        bash_command='echo \"This is the first task.\"'\n",
    "    )\n",
    "\n",
    "    # Task 2: Run another bash command\n",
    "    task2 = BashOperator(\n",
    "        task_id='second_task',\n",
    "        bash_command='echo \"This is the second task.\"'\n",
    "    )\n",
    "\n",
    "    # Task 3: Yet another bash task\n",
    "    task3 = BashOperator(\n",
    "        task_id='third_task',\n",
    "        bash_command='echo \"This is the third task.\"'\n",
    "    )\n",
    "\n",
    "    # Task 4: Final task\n",
    "    task4 = BashOperator(\n",
    "        task_id='final_task',\n",
    "        bash_command='echo \"Workflow complete!\"'\n",
    "    )\n",
    "\n",
    "    # Define task dependencies using bitshift operators\n",
    "    start >> task1 >> task2 >> task3 >> task4  # Linear dependency chain\n",
    "\n",
    "    # Alternatively (equivalent way):\n",
    "    # task1 >> task2\n",
    "    # task2 >> task3\n",
    "    # task3 >> task4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Concepts Highlighted**\n",
    "| Concept                    | Description                                                                 |\n",
    "|---------------------------|-----------------------------------------------------------------------------|\n",
    "| **Tasks = Operators**     | Each `task_id` assigned to a variable (e.g., `task1`) is an instantiated operator. |\n",
    "| **Dependencies**          | `task1 >> task2` means task1 must finish before task2 starts (upstream).      |\n",
    "| **UI Visualization**      | These will be visible in **Graph View** with arrows showing flow between tasks. |\n",
    "| **start & final_task**    | Use of `EmptyOperator` as \"bookends\" is common to clarify flow visually.      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e87d0",
   "metadata": {},
   "source": [
    "### **Define order of BashOperators**\n",
    "Now that you've learned about the bitshift operators, it's time to modify your workflow to include a pull step and to include the task ordering. You have three currently defined components, cleanup, consolidate, and push_data.\n",
    "\n",
    "The DAG analytics_dag is available as before and the BashOperator is already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "with DAG('analytics_dag', start_date=datetime(2023, 4, 1), schedule_interval='@daily') as dag:\n",
    "    \n",
    "    # Define the pull_sales task (first step)\n",
    "    pull_sales = BashOperator(\n",
    "        task_id='pull_sales',\n",
    "        bash_command='wget https://salestracking/latestinfo?json',\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    "    # Define the cleanup task (second step)\n",
    "    cleanup = BashOperator(\n",
    "        task_id='cleanup',\n",
    "        bash_command='rm -rf /tmp/sales_data/*',  # Example cleanup command\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    "    # Define the consolidate task (third step)\n",
    "    consolidate = BashOperator(\n",
    "        task_id='consolidate',\n",
    "        bash_command='python /scripts/consolidate_data.py',  # Example consolidate command\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    "    # Define the push_data task (fourth and last step)\n",
    "    push_data = BashOperator(\n",
    "        task_id='push_data',\n",
    "        bash_command='python /scripts/push_to_database.py',  # Example push data command\n",
    "        dag=dag\n",
    "    )\n",
    "    \n",
    " \n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6e1d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **Check for Import Errors with `list-import-errors`:**\n",
    "To check for any import issues related to your DAGs, run the following command:\n",
    "\n",
    "```bash\n",
    "airflow dags list-import-errors\n",
    "```\n",
    "\n",
    "This command will provide a detailed list of all import errors that Airflow encounters while trying to load the DAGs. It will point out exactly where the issues are, whether it's due to a missing module, a wrong file path, or other import-related issues.\n",
    "\n",
    "### 2. **Decipher the Error Message:**\n",
    "The output will include the error type and location. For example, you might see an error like:\n",
    "\n",
    "```\n",
    "ImportError: cannot import name 'MyCustomOperator' from 'my_custom_module'\n",
    "```\n",
    "\n",
    "This indicates that Airflow is unable to import `MyCustomOperator` from the specified `my_custom_module`, likely because the module is missing, the file path is incorrect, or the class isn't defined properly in the module.\n",
    "\n",
    "### 3. **View the Python Code:**\n",
    "After identifying the import error, you can view the Python code to understand the issue better. Run the following command to check the DAG's code:\n",
    "\n",
    "```bash\n",
    "cat workspace/dags/codependent.py\n",
    "```\n",
    "\n",
    "In the file, look at the import statements and ensure they are correct. Verify if the module paths are accurate and that all the required libraries or custom modules are available.\n",
    "\n",
    "For example, check if you have something like this in your code:\n",
    "\n",
    "```python\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "```\n",
    "\n",
    "If you're using custom operators or modules, ensure they're correctly imported and available to Airflow. Double-check the paths to avoid any typos or mistakes.\n",
    "\n",
    "### 4. **Fix the Import Error:**\n",
    "Once you've identified the problem, make the necessary corrections:\n",
    "- Ensure all custom modules are properly installed or available in the right directory.\n",
    "- Install any missing dependencies with `pip`.\n",
    "- Double-check the import paths for accuracy.\n",
    "\n",
    "For example, if a custom module is missing, you can install it using:\n",
    "\n",
    "```bash\n",
    "pip install <missing_module>\n",
    "```\n",
    "\n",
    "### 5. **Re-run the DAG:**\n",
    "Once the import issues are resolved, you can trigger the DAG again to ensure everything is working as expected:\n",
    "\n",
    "```bash\n",
    "airflow dags trigger <dag_id>\n",
    "```\n",
    "\n",
    "### Summary of Commands for Troubleshooting Import Errors:\n",
    "- **List Import Errors**: `airflow dags list-import-errors`\n",
    "- **View the DAG Code**: `cat workspace/dags/codependent.py`\n",
    "- **Fix Import Issues**: Correct the import paths, install missing dependencies, and ensure custom modules are available.\n",
    "- **Re-run the DAG**: `airflow dags trigger <dag_id>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974e88a",
   "metadata": {},
   "source": [
    "### Summary of Key Points: Additional Operators in Airflow\n",
    "\n",
    "1. **PythonOperator**  \n",
    "   The `PythonOperator` is used to execute a Python function or callable method as part of an Airflow task. It requires the `task_id`, `dag`, and `python_callable` arguments, with the `python_callable` being the function you wish to run. Arguments can be passed to the function using `op_args` (for positional arguments) or `op_kwargs` (for keyword arguments).\n",
    "\n",
    "   - **Key Arguments**:\n",
    "     - `task_id`: Unique identifier for the task.\n",
    "     - `dag`: The DAG this task belongs to.\n",
    "     - `python_callable`: The Python function to execute.\n",
    "     - `op_args`: Positional arguments passed to the function.\n",
    "     - `op_kwargs`: Keyword arguments passed to the function.\n",
    "\n",
    "2. **PythonOperator Example**  \n",
    "   An example shows how to use the `PythonOperator` to run a simple function `printme`, which writes a message to the task logs.\n",
    "\n",
    "   ```python\n",
    "   from airflow.operators.python import PythonOperator\n",
    "\n",
    "   def printme():\n",
    "       print(\"Hello from PythonOperator!\")\n",
    "\n",
    "   task = PythonOperator(\n",
    "       task_id='python_task',\n",
    "       python_callable=printme,\n",
    "       dag=dag\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Arguments in PythonOperator**  \n",
    "   You can pass arguments to the function via `op_args` and `op_kwargs`. For example, using `op_kwargs`, you can pass named arguments to the function.\n",
    "\n",
    "4. **op_kwargs Example**  \n",
    "   The example defines a `sleep` function that accepts a `length_of_time` argument and then pauses for that amount of time. The `op_kwargs` dictionary is used to pass the `length_of_time` argument to the function.\n",
    "\n",
    "   ```python\n",
    "   from airflow.operators.python import PythonOperator\n",
    "   import time\n",
    "\n",
    "   def sleep(length_of_time):\n",
    "       time.sleep(length_of_time)\n",
    "\n",
    "   task = PythonOperator(\n",
    "       task_id='sleep_task',\n",
    "       python_callable=sleep,\n",
    "       op_kwargs={'length_of_time': 5},\n",
    "       dag=dag\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   - **Key Notes**: The dictionary keys must match the function's argument names. Incorrect keys can result in errors like `unexpected keyword argument`.\n",
    "\n",
    "5. **EmailOperator**  \n",
    "   The `EmailOperator` is used to send emails from within an Airflow task. This is useful for sending notifications, reports, etc. It requires the `task_id`, `to`, `subject`, and `html_content` arguments. The system must be configured with email server details for this to work.\n",
    "\n",
    "   - **Key Arguments**:\n",
    "     - `task_id`: Unique identifier for the task.\n",
    "     - `to`: Recipient's email address.\n",
    "     - `subject`: Subject of the email.\n",
    "     - `html_content`: HTML content of the email body.\n",
    "     - `files`: Attachments (optional).\n",
    "\n",
    "6. **EmailOperator Example**  \n",
    "   In this example, the `EmailOperator` sends an email with a sales report attached, once the task is completed.\n",
    "\n",
    "   ```python\n",
    "   from airflow.operators.email import EmailOperator\n",
    "\n",
    "   task = EmailOperator(\n",
    "       task_id='send_email',\n",
    "       to='recipient@example.com',\n",
    "       subject='Sales Report',\n",
    "       html_content='Please find the sales report attached.',\n",
    "       files=['/path/to/latest_sales.xlsx'],\n",
    "       dag=dag\n",
    "   )\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Common Command-Line Operations for These Operators\n",
    "\n",
    "- **List all DAGs**: `airflow dags list`\n",
    "- **Trigger a DAG**: `airflow dags trigger <dag_id>`\n",
    "- **List task instances in a DAG**: `airflow tasks list <dag_id>`\n",
    "- **Check task status**: `airflow tasks state <dag_id> <task_id> <execution_date>`\n",
    "- **View DAG logs**: `airflow logs <dag_id> <task_id> <execution_date>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324fb7a",
   "metadata": {},
   "source": [
    "### Additional Operators in Airflow\n",
    "\n",
    "1. **PythonOperator**  \n",
    "   The `PythonOperator` is used to execute a Python function or callable method as part of an Airflow task. It requires the `task_id`, `dag`, and `python_callable` arguments, with the `python_callable` being the function you wish to run. Arguments can be passed to the function using `op_args` (for positional arguments) or `op_kwargs` (for keyword arguments).\n",
    "\n",
    "   - **Key Arguments**:\n",
    "     - `task_id`: Unique identifier for the task.\n",
    "     - `dag`: The DAG this task belongs to.\n",
    "     - `python_callable`: The Python function to execute.\n",
    "     - `op_args`: Positional arguments passed to the function.\n",
    "     - `op_kwargs`: Keyword arguments passed to the function.\n",
    "\n",
    "2. **PythonOperator Example**  \n",
    "   An example shows how to use the `PythonOperator` to run a simple function `printme`, which writes a message to the task logs.\n",
    "\n",
    "   ```python\n",
    "   from airflow.operators.python import PythonOperator\n",
    "\n",
    "   def printme():\n",
    "       print(\"Hello from PythonOperator!\")\n",
    "\n",
    "   task = PythonOperator(\n",
    "       task_id='python_task',\n",
    "       python_callable=printme,\n",
    "       dag=dag\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Arguments in PythonOperator**  \n",
    "   You can pass arguments to the function via `op_args` and `op_kwargs`. For example, using `op_kwargs`, you can pass named arguments to the function.\n",
    "\n",
    "4. **op_kwargs Example**  \n",
    "   The example defines a `sleep` function that accepts a `length_of_time` argument and then pauses for that amount of time. The `op_kwargs` dictionary is used to pass the `length_of_time` argument to the function.\n",
    "\n",
    "   ```python\n",
    "   from airflow.operators.python import PythonOperator\n",
    "   import time\n",
    "\n",
    "   def sleep(length_of_time):\n",
    "       time.sleep(length_of_time)\n",
    "\n",
    "   task = PythonOperator(\n",
    "       task_id='sleep_task',\n",
    "       python_callable=sleep,\n",
    "       op_kwargs={'length_of_time': 5},\n",
    "       dag=dag\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   - **Key Notes**: The dictionary keys must match the function's argument names. Incorrect keys can result in errors like `unexpected keyword argument`.\n",
    "\n",
    "5. **EmailOperator**  \n",
    "   The `EmailOperator` is used to send emails from within an Airflow task. This is useful for sending notifications, reports, etc. It requires the `task_id`, `to`, `subject`, and `html_content` arguments. The system must be configured with email server details for this to work.\n",
    "\n",
    "   - **Key Arguments**:\n",
    "     - `task_id`: Unique identifier for the task.\n",
    "     - `to`: Recipient's email address.\n",
    "     - `subject`: Subject of the email.\n",
    "     - `html_content`: HTML content of the email body.\n",
    "     - `files`: Attachments (optional).\n",
    "\n",
    "6. **EmailOperator Example**  \n",
    "   In this example, the `EmailOperator` sends an email with a sales report attached, once the task is completed.\n",
    "\n",
    "   ```python\n",
    "   from airflow.operators.email import EmailOperator\n",
    "\n",
    "   task = EmailOperator(\n",
    "       task_id='send_email',\n",
    "       to='recipient@example.com',\n",
    "       subject='Sales Report',\n",
    "       html_content='Please find the sales report attached.',\n",
    "       files=['/path/to/latest_sales.xlsx'],\n",
    "       dag=dag\n",
    "   )\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Common Command-Line Operations for These Operators\n",
    "\n",
    "- **List all DAGs**: `airflow dags list`\n",
    "- **Trigger a DAG**: `airflow dags trigger <dag_id>`\n",
    "- **List task instances in a DAG**: `airflow tasks list <dag_id>`\n",
    "- **Check task status**: `airflow tasks state <dag_id> <task_id> <execution_date>`\n",
    "- **View DAG logs**: `airflow logs <dag_id> <task_id> <execution_date>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba00b4",
   "metadata": {},
   "source": [
    "### **Using the PythonOperator**\n",
    "You've implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You'll implement a task to download and save a file to the system within Airflow.\n",
    "\n",
    "The requests library is imported for you, and the DAG process_sales_dag is already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f630df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)    \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3850a7",
   "metadata": {},
   "source": [
    "### **Using the PythonOperator**\n",
    "You've implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You'll implement a task to download and save a file to the system within Airflow.\n",
    "\n",
    "The requests library is imported for you, and the DAG process_sales_dag is already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299f735",
   "metadata": {},
   "source": [
    "### **EmailOperator and dependencies**\n",
    "Now that you've successfully defined the PythonOperators for your workflow, your manager would like to receive a copy of the parsed JSON file via email when the workflow completes. The previous tasks are still defined and the DAG process_sales_dag is configured. Please note that this task uses the older DAG definition method and is added for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618bfa5",
   "metadata": {},
   "source": [
    "### Summary of Key Points: Airflow Scheduling\n",
    "\n",
    "1. **DAG Runs**  \n",
    "   A DAG run refers to a specific execution instance of a workflow in Airflow. It can be triggered manually or via the schedule interval defined for the DAG. Each DAG run maintains its own state (running, failed, or success) and can have individual task states (queued, skipped, etc.).\n",
    "\n",
    "2. **DAG Runs View**  \n",
    "   In the Airflow UI, the **Browse: DAG Runs** menu option allows you to view all DAG runs. You can check the status and details of each DAG run in this view.\n",
    "\n",
    "3. **DAG Run State**  \n",
    "   The state of each DAG run is displayed, indicating whether the run was successful, failed, or had other states. This is useful for monitoring and troubleshooting DAGs.\n",
    "\n",
    "4. **Schedule Details**  \n",
    "   When scheduling a DAG, several parameters are important:\n",
    "   - **Start Date**: The first possible time for the DAG to run (typically a Python `datetime` object).\n",
    "   - **End Date**: The last possible time for the DAG to run.\n",
    "   - **Max Tries**: The number of times Airflow should attempt to rerun the DAG before considering it as failed.\n",
    "   - **Schedule Interval**: How often the DAG runs.\n",
    "\n",
    "5. **Schedule Interval**  \n",
    "   The schedule interval determines how frequently the DAG should run. This is not the exact time it will run, but the window in which it could be scheduled. The interval can be set using cron syntax or predefined presets.\n",
    "\n",
    "6. **Cron Syntax**  \n",
    "   Airflow supports cron-style syntax to define the schedule interval. This syntax includes five fields:\n",
    "   - Minute (0-59)\n",
    "   - Hour (0-23)\n",
    "   - Day of the month (1-31)\n",
    "   - Month (1-12)\n",
    "   - Day of the week (0-6)\n",
    "   \n",
    "   Asterisks (*) represent all values in a field (e.g., `*` in the minute field means every minute). You can also specify a list of values (e.g., `0,15,30,45` for every 15 minutes).\n",
    "\n",
    "7. **Cron Examples**  \n",
    "   - `0 12 * * *`: Runs daily at Noon (12:00).\n",
    "   - `25 2 * * *`: Runs once per minute on February 25th.\n",
    "   - `0,15,30,45 * * * *`: Runs every 15 minutes.\n",
    "\n",
    "8. **Airflow Scheduler Presets**  \n",
    "   Airflow provides preset options for common time intervals:\n",
    "   - `@hourly`: Equivalent to `0 * * * *` in cron, runs once an hour at the start of the hour.\n",
    "   - `@daily`: Runs once a day at midnight.\n",
    "   - `@weekly`: Runs once a week on Sunday at midnight.\n",
    "   - `@monthly`: Runs once a month on the first day at midnight.\n",
    "   - `@yearly`: Runs once a year on January 1st at midnight.\n",
    "\n",
    "9. **Special Presets**  \n",
    "   Airflow includes two special presets:\n",
    "   - `None`: Means the DAG will not be scheduled, typically used for manually triggered workflows.\n",
    "   - `@once`: Schedules the DAG to run only once.\n",
    "\n",
    "10. **Schedule Interval Nuances**  \n",
    "    A key scheduling nuance: when defining a **start_date**, Airflow will not schedule the first run until at least one schedule interval has passed after the start date. For example, if the start date is set to February 25, 2020, and the schedule interval is `@daily`, the first DAG run will be scheduled for February 26, 2020.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Airflow Scheduling Commands\n",
    "\n",
    "- **List all DAGs**: `airflow dags list`\n",
    "- **Trigger a DAG**: `airflow dags trigger <dag_id>`\n",
    "- **List DAG runs**: `airflow dags list-runs <dag_id>`\n",
    "- **Check task state**: `airflow tasks state <dag_id> <task_id> <execution_date>`\n",
    "- **View logs**: `airflow logs <dag_id> <task_id> <execution_date>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392905e",
   "metadata": {},
   "source": [
    "Here is a neat table summarizing the key points related to **Airflow Scheduling**:\n",
    "\n",
    "| **Topic**                     | **Details**                                                                                                            |\n",
    "|-------------------------------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| **DAG Runs**                   | A DAG run refers to a specific execution instance of a workflow in Airflow. Can be triggered manually or via a schedule.  |\n",
    "| **DAG Runs View**              | In the Airflow UI, under **Browse > DAG Runs**, you can view details and states of all DAG runs in the instance.        |\n",
    "| **DAG Run State**              | Displays the state of each DAG run: running, failed, success, etc. Useful for monitoring and troubleshooting.           |\n",
    "| **Schedule Details**           | Key parameters: <br> - **Start Date**: First possible time for DAG to run. <br> - **End Date**: Last possible time. <br> - **Max Tries**: Max retries before failure. <br> - **Schedule Interval**: Frequency of DAG runs. |\n",
    "| **Schedule Interval**          | Defines how frequently the DAG runs. Can be set with cron syntax or predefined presets.                               |\n",
    "| **Cron Syntax**                | Standard cron format: `minute hour day_of_month month day_of_week`. Asterisks (`*`) mean every possible value.          |\n",
    "| **Cron Examples**              | - `0 12 * * *`: Run daily at Noon.<br> - `25 2 * * *`: Run on February 25th every minute.<br> - `0,15,30,45 * * * *`: Run every 15 minutes. |\n",
    "| **Airflow Scheduler Presets**  | Predefined scheduling shortcuts:<br> - `@hourly`: Runs once every hour.<br> - `@daily`: Runs once per day.<br> - `@weekly`: Runs once per week.<br> - `@monthly`: Runs once per month.<br> - `@yearly`: Runs once a year. |\n",
    "| **Special Presets**            | - `None`: No scheduling, used for manual execution.<br> - `@once`: Schedules the DAG to run only once.                   |\n",
    "| **Schedule Interval Nuances**  | Airflow won‚Äôt schedule the first run until one schedule interval has passed after the **start_date**.                   |\n",
    "\n",
    "### Common Airflow Scheduling Commands\n",
    "\n",
    "| **Command**                           | **Description**                                                   |\n",
    "|---------------------------------------|-------------------------------------------------------------------|\n",
    "| `airflow dags list`                   | Lists all available DAGs in the Airflow instance.                 |\n",
    "| `airflow dags trigger <dag_id>`       | Manually triggers a DAG run.                                      |\n",
    "| `airflow dags list-runs <dag_id>`     | Lists the runs of a specific DAG.                                 |\n",
    "| `airflow tasks state <dag_id> <task_id> <execution_date>` | Checks the state of a task for a specific DAG run.                |\n",
    "| `airflow logs <dag_id> <task_id> <execution_date>` | Displays the logs for a specific task run in a DAG.               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7574d",
   "metadata": {},
   "source": [
    "### **Schedule a DAG via Python**\n",
    "You've learned quite a bit about creating DAGs, but now you would like to schedule a specific DAG on a specific day of the week at a certain time. You'd like the code include this information in case a colleague needs to reinstall the DAG to a different server.\n",
    "\n",
    "The Airflow DAG object and the appropriate datetime methods have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2023, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef53057",
   "metadata": {},
   "source": [
    "### **Deciphering Airflow schedules**\n",
    "Given the various options for Airflow's schedule_interval, you'd like to verify that you understand exactly how intervals relate to each other, whether it's a cron format, timedelta object, or a preset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bb036",
   "metadata": {},
   "source": [
    "Here‚Äôs a comprehensive table that highlights the use of `*` (asterisk) in various scheduling formats in Airflow, including Cron Syntax, Timedelta, and Presets.\n",
    "\n",
    "### **Airflow Schedule Formats: The Role of `*` (Asterisk)**\n",
    "\n",
    "| **Schedule Format** | **Description**                                           | **Example Usage**                     | **Explanation**                                                        |\n",
    "|---------------------|-----------------------------------------------------------|---------------------------------------|------------------------------------------------------------------------|\n",
    "| **Cron Syntax**      | The `*` in Cron syntax means \"every possible value\" for that field. | `* * * * *`                           | Run **every minute** of every hour, day, month, and week.              |\n",
    "| **Minute Field**     | The first field (0-59) specifies the minute of the hour.     | `* 12 * * *`                          | Run **every minute** at **12 PM** each day.                            |\n",
    "| **Hour Field**       | The second field (0-23) specifies the hour of the day.       | `0 * * * *`                           | Run **at the start of every hour** each day.                           |\n",
    "| **Day of Month**     | The third field (1-31) specifies the day of the month.       | `0 0 * * *`                           | Run **at midnight** every day of the month.                            |\n",
    "| **Month Field**      | The fourth field (1-12) specifies the month of the year.    | `0 0 * 5 *`                           | Run **at midnight** every day in **May**.                              |\n",
    "| **Day of Week**      | The fifth field (0-6, 0=Sunday) specifies the day of the week. | `0 0 * * 0`                         | Run **at midnight every Sunday**.                                      |\n",
    "| **Timedelta (days)** | In Timedelta, `*` is not used directly, but a value for a unit is specified. | `timedelta(days=1)`                  | Run **every day** (this is equivalent to a `@daily` preset).           |\n",
    "| **Timedelta (hours)**| Specifies the number of hours between runs.                 | `timedelta(hours=1)`                  | Run **every hour**.                                                    |\n",
    "| **Presets**          | The `*` in presets indicates an automatic, predefined schedule. | `@hourly`, `@daily`, `@weekly`, etc. | These are shortcuts for common schedules, and `*` is handled internally. |\n",
    "| **@hourly**          | Run once **every hour**                                    | `@hourly`                             | Equivalent to `0 * * * *` (Run every hour at the start of the hour).   |\n",
    "| **@daily**           | Run once **every day**                                     | `@daily`                              | Equivalent to `0 0 * * *` (Run at midnight every day).                |\n",
    "| **@weekly**          | Run once **every week**                                    | `@weekly`                             | Equivalent to `0 0 * * 0` (Run at midnight on Sunday).                |\n",
    "| **@monthly**         | Run once **every month**                                   | `@monthly`                            | Equivalent to `0 0 1 * *` (Run at midnight on the first of every month). |\n",
    "| **@yearly**          | Run once **every year**                                    | `@yearly`                             | Equivalent to `0 0 1 1 *` (Run at midnight on January 1st).            |\n",
    "| **None**             | No schedule, manual trigger only.                          | `None`                                | The DAG will **never** run automatically.                              |\n",
    "| **@once**            | Run the DAG **once**, then stop scheduling.                | `@once`                               | The DAG will run only one time, not on a recurring schedule.           |\n",
    "\n",
    "### **Explanation of `*` in Cron Syntax**\n",
    "\n",
    "- **Minute Field**: `*` means \"run every minute,\" which makes it highly flexible for tasks requiring frequent execution.\n",
    "- **Hour Field**: `*` means \"run every hour,\" ensuring tasks are executed every hour regardless of the specific time.\n",
    "- **Day of Month**: `*` ensures the task runs every day of the month. This is useful for daily tasks that need to run at the same time each day.\n",
    "- **Month Field**: `*` ensures the task runs every month. For example, `* * * 5 *` runs the task every day in May.\n",
    "- **Day of Week**: `*` indicates the task should run on any day of the week, while specific numbers (e.g., `0` for Sunday) would restrict execution to a specific weekday.\n",
    "\n",
    "### **Special Considerations**\n",
    "- **Use of `*` in Presets**: For presets like `@hourly`, `@daily`, `@weekly`, etc., the `*` is handled internally and is not explicitly part of the user-defined syntax. These are shorthand expressions for common scheduling patterns.\n",
    "- **Manual Scheduling with `None`**: If `schedule_interval` is set to `None`, the DAG will not have any automatic schedule. This is ideal for workflows that should only be triggered manually.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like further clarification on any specific schedule format or example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2f376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
