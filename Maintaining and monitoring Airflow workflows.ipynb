{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8fe6f5",
   "metadata": {},
   "source": [
    "Here's an updated table with the command examples included for **Airflow Sensors**:\n",
    "\n",
    "| **Topic**                      | **Details**                                                                                                                                                                      | **Command Example**                                                                                     |\n",
    "|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| **What are Sensors?**           | Sensors are special operators that wait for a certain condition to be true, such as the existence of a file, a record in a database, or a response from a web request.            | No specific command, but you define sensors as tasks in the DAG, like any other operator.               |\n",
    "| **Sensor Details**              | Derived from `BaseSensorOperator`. Common arguments:                                                                                                                             |                                                                                                         |\n",
    "|                                 | - **Mode**: `poke` (default, keeps checking until the condition is true) or `reschedule` (gives up worker slot and waits for another slot).                                        | No specific command, but when defining the sensor task, you can set `mode`, `poke_interval`, and `timeout`. |\n",
    "|                                 | - **poke_interval**: How often to check the condition (typically at least 1 minute).                                                                                             | `poke_interval=300` (every 5 minutes)                                                                    |\n",
    "|                                 | - **timeout**: How long to wait (in seconds) before marking the sensor task as failed.                                                                                            | `timeout=600` (wait 10 minutes before failing)                                                          |\n",
    "|                                 | These sensors can also have other operator attributes like `task_id` and `dag`.                                                                                                  | `task_id=\"file_sensor_task\", dag=dag_name`                                                              |\n",
    "| **File Sensor**                 | The `FileSensor` checks for the existence of a file at a specified path. Example: Wait for `salesdata.csv` file to appear before continuing.                                       | ```from airflow.sensors.filesystem import FileSensor                                                            |\n",
    "|                                 |                                                                                                                                                                                  | file_sensor_task = FileSensor(                                        |\n",
    "|                                 |                                                                                                                                                                                  |     task_id='file_sensor_task',                                              |\n",
    "|                                 |                                                                                                                                                                                  |     filepath='/path/to/salesdata.csv',                                             |\n",
    "|                                 |                                                                                                                                                                                  |     poke_interval=300,                                                              |\n",
    "|                                 |                                                                                                                                                                                  |     timeout=600,                                                                  |\n",
    "|                                 |                                                                                                                                                                                  |     dag=process_sales_dag                                                   )``` |\n",
    "| **Other Sensors**               | - **ExternalTaskSensor**: Waits for a task in another DAG to complete.                                                                                                          | ```from airflow.sensors.external_task import ExternalTaskSensor                                               |\n",
    "|                                 |                                                                                                                                                                                  | external_task_sensor = ExternalTaskSensor(                                                 |\n",
    "|                                 |                                                                                                                                                                                  |     task_id='external_task_check',                                               |\n",
    "|                                 |                                                                                                                                                                                  |     external_dag_id='other_dag_id',                                              |\n",
    "|                                 |                                                                                                                                                                                  |     external_task_id='task_in_other_dag',                                         |\n",
    "|                                 |                                                                                                                                                                                  |     timeout=600,                                                                  |\n",
    "|                                 |                                                                                                                                                                                  |     poke_interval=300,                                                            |\n",
    "|                                 |                                                                                                                                                                                  |     dag=process_sales_dag                                                   )``` |\n",
    "|                                 | - **HttpSensor**: Waits for a response from a web URL and checks for specific content.                                                                                           | ```from airflow.sensors.http import HttpSensor                                                              |\n",
    "|                                 |                                                                                                                                                                                  | http_sensor_task = HttpSensor(                                                           |\n",
    "|                                 |                                                                                                                                                                                  |     task_id='http_check',                                                             |\n",
    "|                                 |                                                                                                                                                                                  |     http_conn_id='http_default',                                                      |\n",
    "|                                 |                                                                                                                                                                                  |     endpoint='api/v1/data',                                                           |\n",
    "|                                 |                                                                                                                                                                                  |     poke_interval=300,                                                                |\n",
    "|                                 |                                                                                                                                                                                  |     timeout=600,                                                                      |\n",
    "|                                 |                                                                                                                                                                                  |     response_check=lambda response: \"data\" in response.text,                           |\n",
    "|                                 |                                                                                                                                                                                  |     dag=process_sales_dag                                                         )``` |\n",
    "|                                 | - **SqlSensor**: Executes an SQL query to check for content.                                                                                                                     | ```from airflow.sensors.sql import SqlSensor                                                                 |\n",
    "|                                 |                                                                                                                                                                                  | sql_sensor_task = SqlSensor(                                                          |\n",
    "|                                 |                                                                                                                                                                                  |     task_id='sql_check',                                                             |\n",
    "|                                 |                                                                                                                                                                                  |     sql='SELECT COUNT(*) FROM sales WHERE status = \"complete\";',                     |\n",
    "|                                 |                                                                                                                                                                                  |     poke_interval=300,                                                                |\n",
    "|                                 |                                                                                                                                                                                  |     timeout=600,                                                                      |\n",
    "|                                 |                                                                                                                                                                                  |     conn_id='db_connection',                                                        |\n",
    "|                                 |                                                                                                                                                                                  |     mode='poke',                                                                     |\n",
    "|                                 |                                                                                                                                                                                  |     dag=process_sales_dag                                                           )``` |\n",
    "| **When to Use Sensors**         | - When you're uncertain when a condition will be true (e.g., a file appearing, a database record being updated).                                                                 | No specific command, but you will use the sensor in situations where conditions may vary in timing.     |\n",
    "|                                 | - When you want to check for a condition multiple times but avoid failing the entire DAG immediately.                                                                            | You can use the **poke** mode and adjust `poke_interval` and `timeout` as needed.                       |\n",
    "|                                 | - If you want to run a check repeatedly without consuming resources from the DAG directly.                                                                                       | Using **reschedule** mode instead of **poke** mode helps free up worker resources.                       |\n",
    "\n",
    "### Additional Information:\n",
    "- **Poke mode** checks for conditions continuously but holds onto the worker slot.\n",
    "- **Reschedule mode** releases the worker slot and tries to check again after the `poke_interval` without blocking the system.\n",
    "\n",
    "Let me know if you need further clarification!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83bbbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Airflow Executors**\n",
    "\n",
    "| **Topic**                     | **Details**                                                                                                                                                                                                                                                                             | **Command Example** / **Config Path**                                                                 |\n",
    "|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| **What is an Executor?**     | An executor is the Airflow component responsible for **running tasks** defined in DAGs. Each type of executor has different behavior and capabilities—some run tasks sequentially, others concurrently across multiple machines or containers.                                               | N/A                                                                                                    |\n",
    "| **SequentialExecutor**       | - **Default executor** in Airflow.<br>- Executes **only one task at a time**.<br>- Good for **debugging and learning**.<br>- **Not suitable** for production as it lacks parallelism.                                                                                                   | `executor = SequentialExecutor`<br>in `airflow.cfg` file                                               |\n",
    "| **LocalExecutor**            | - Runs **multiple tasks concurrently** on a **single machine**.<br>- Each task runs as a separate **local process**.<br>- You can set **parallelism** limit.<br>- Suitable for **small production systems** on a single host.                                                              | `executor = LocalExecutor`<br>in `airflow.cfg` file<br>Configure `parallelism`, `dag_concurrency`     |\n",
    "| **KubernetesExecutor**       | - Runs tasks in **Kubernetes pods**, enabling distributed execution.<br>- Supports **dynamic scaling** by adding/removing worker nodes.<br>- Requires **Kubernetes cluster**, shared DAG storage (like NFS or Git), and extra configuration.<br>- Best for **large-scale, scalable systems**. | `executor = KubernetesExecutor`<br>in `airflow.cfg`<br>Needs proper Kubernetes setup beforehand        |\n",
    "| **Determine Executor (cfg)** | Check the **executor type** by opening the Airflow config file. Look for the `executor = ...` line.                                                                                                                                                                                      | `cat $AIRFLOW_HOME/airflow.cfg \\| grep executor`                                                       |\n",
    "| **Determine Executor (CLI)** | You can also determine the executor via CLI using `airflow info`. It shows detailed environment settings including the executor in use.                                                                                                                                                | `airflow info` → Look for the `executor` field in the output                                          |\n",
    "\n",
    "### ⚠️ Notes:\n",
    "- If you're using Docker or Kubernetes, the `airflow.cfg` file may be mounted or managed differently (e.g., via Helm in Kubernetes).\n",
    "- You may also want to check related config keys: `parallelism`, `dag_concurrency`, `max_active_runs_per_dag`, and `worker_concurrency`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a9e03",
   "metadata": {},
   "source": [
    "Here's a clean **comparison chart** to help you decide **which Airflow executor to use** based on your needs:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 **Airflow Executors Comparison Table**\n",
    "\n",
    "| **Feature**                 | **SequentialExecutor**                      | **LocalExecutor**                                | **KubernetesExecutor**                                           |\n",
    "|----------------------------|---------------------------------------------|--------------------------------------------------|------------------------------------------------------------------|\n",
    "| **Execution Type**         | Single task at a time                       | Parallel tasks as local processes                | Distributed tasks in Kubernetes pods                             |\n",
    "| **Concurrency**            | ❌ No concurrency                           | ✅ Concurrent (depends on CPU/memory & config)   | ✅ Massive concurrency (scales with cluster size)                |\n",
    "| **Best For**               | Learning, debugging                         | Small to medium prod workloads                   | Large-scale, scalable prod environments                          |\n",
    "| **Setup Complexity**       | 🟢 Simple                                    | 🟡 Moderate                                       | 🔴 Complex (needs full Kubernetes setup)                          |\n",
    "| **Resource Usage**         | Minimal                                     | High (CPU/mem of host)                           | Depends on Kubernetes cluster                                    |\n",
    "| **Scalability**            | ❌ Not scalable                             | 🟡 Limited (only one machine)                    | ✅ Auto-scalable via Kubernetes                                  |\n",
    "| **Config File Setting**    | `executor = SequentialExecutor`             | `executor = LocalExecutor`                       | `executor = KubernetesExecutor`                                  |\n",
    "| **Supports Parallel DAGs** | ❌ No                                        | ✅ Yes                                            | ✅ Yes                                                            |\n",
    "| **Dag Deployment Method**  | Local filesystem                            | Local filesystem                                 | Git sync, NFS, S3, etc.                                          |\n",
    "| **Use in Production**      | ❌ Not recommended                          | ✅ Yes (for simple, single-host systems)         | ✅ Yes (recommended for dynamic/large systems)                   |\n",
    "| **Failover/Resilience**    | ❌ No                                        | 🟡 Some (depends on host reliability)            | ✅ Yes (Kubernetes handles pod restarts and failures)            |\n",
    "| **Airflow CLI Command**    | `airflow info` or check `airflow.cfg`       | Same                                             | Same (plus K8s setup like Helm or kubectl may apply)            |\n",
    "\n",
    "---\n",
    "\n",
    "### 🚦**Which one should you choose?**\n",
    "\n",
    "| **Scenario**                                     | **Recommended Executor**   |\n",
    "|--------------------------------------------------|----------------------------|\n",
    "| Learning or testing on a local machine           | `SequentialExecutor`       |\n",
    "| Running a few DAGs on a VM or a single server    | `LocalExecutor`            |\n",
    "| Running large pipelines or scaling dynamically   | `KubernetesExecutor`       |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889c03e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🛠️ **Airflow Debugging & Troubleshooting Summary**\n",
    "\n",
    "| **Issue**                        | **Description**                                                                 | **Resolution** / **Command**                                                                 |\n",
    "|----------------------------------|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **DAG won't run on schedule**    | - Scheduler isn't running<br>- Start date hasn't passed<br>- Executor slots full | ✅ Start scheduler: `airflow scheduler`<br>🛠 Adjust start_date/schedule<br>🛠 Change executor or add resources |\n",
    "| **DAG not loading in UI**        | - Python file not in `dags_folder`<br>- Wrong file path                          | 🔍 Check path in `airflow.cfg`: `dags_folder` (should be absolute)<br>✅ Place script there   |\n",
    "| **Syntax errors in DAG file**    | - Python code has errors                                                        | 🔍 Run: `python3 your_dag.py`<br>🔍 Or: `airflow dags list-import-errors`                     |\n",
    "| **Import errors (not visible DAGs)** | - Airflow can't parse DAG file due to import or syntax issues                 | ✅ Run: `airflow dags list-import-errors`                                                     |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **Helpful Airflow Commands**\n",
    "\n",
    "| **Command**                              | **Purpose**                                                     |\n",
    "|------------------------------------------|-----------------------------------------------------------------|\n",
    "| `airflow scheduler`                      | Starts the Airflow scheduler (needed to run DAGs)               |\n",
    "| `airflow dags list`                      | Lists all available DAGs                                       |\n",
    "| `airflow dags list-import-errors`        | Shows syntax/import errors in DAG files                         |\n",
    "| `python3 your_dag.py`                    | Checks for syntax errors by running the DAG file as Python      |\n",
    "| `airflow info`                           | Displays current environment setup including executor, paths    |\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Checklist for Common Issues**\n",
    "\n",
    "- [ ] Is the **scheduler** running?\n",
    "- [ ] Did the **start_date** already pass?\n",
    "- [ ] Does your **executor** have enough slots?\n",
    "- [ ] Is your DAG file placed in the **correct DAGs folder**?\n",
    "- [ ] Have you run `airflow dags list-import-errors` to find syntax issues?\n",
    "- [ ] Did you try running the DAG Python file manually to spot syntax errors?\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35f8d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 🛠️ `airflow_debugger.sh`\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"🔍 Starting Airflow Debugging Script...\"\n",
    "\n",
    "# Check if airflow scheduler is running\n",
    "echo -e \"\\n📦 Checking if Airflow Scheduler is running...\"\n",
    "if pgrep -f \"airflow scheduler\" > /dev/null\n",
    "then\n",
    "    echo \"✅ Airflow scheduler is running.\"\n",
    "else\n",
    "    echo \"❌ Scheduler is NOT running. Run it using: airflow scheduler\"\n",
    "fi\n",
    "\n",
    "# Show current executor\n",
    "echo -e \"\\n⚙️ Current Executor:\"\n",
    "airflow info | grep -i \"executor\"\n",
    "\n",
    "# Show DAGs folder path\n",
    "echo -e \"\\n📁 DAGs Folder:\"\n",
    "airflow info | grep -i \"dags folder\"\n",
    "\n",
    "# List all loaded DAGs\n",
    "echo -e \"\\n📜 Listing all available DAGs:\"\n",
    "airflow dags list\n",
    "\n",
    "# Check for import errors\n",
    "echo -e \"\\n🚨 Checking for DAG import errors:\"\n",
    "airflow dags list-import-errors\n",
    "\n",
    "# Optional: run syntax check for a specific DAG\n",
    "read -p $'\\n📝 Enter DAG file path for syntax check (leave blank to skip): ' DAG_FILE\n",
    "if [[ -n \"$DAG_FILE\" ]]; then\n",
    "    echo -e \"\\n🧪 Running python syntax check on $DAG_FILE...\"\n",
    "    python3 \"$DAG_FILE\"\n",
    "fi\n",
    "\n",
    "echo -e \"\\n✅ Debugging completed. Happy scheduling! 🚀\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍💻 How to use:\n",
    "\n",
    "1. Save the file: `nano airflow_debugger.sh`\n",
    "2. Paste the script above, save (`Ctrl+O`, then `Enter`) and exit (`Ctrl+X`)\n",
    "3. Make it executable: `chmod +x airflow_debugger.sh`\n",
    "4. Run it anytime: `./airflow_debugger.sh`\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8606057",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧩 What is an SLA in Airflow?\n",
    "- **SLA (Service Level Agreement)** in Airflow = *expected execution time* for a task or DAG.\n",
    "- If a task **exceeds** this time → **SLA miss**:\n",
    "  - Logged in the system.\n",
    "  - Triggers an **email alert** (if configured).\n",
    "  - Viewable via **Web UI → Browse → SLA Misses**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 How to Define an SLA\n",
    "\n",
    "#### 1. Per Task Level (in `PythonOperator`, etc.)\n",
    "```python\n",
    "from datetime import timedelta\n",
    "\n",
    "task1 = PythonOperator(\n",
    "    task_id='my_task',\n",
    "    python_callable=my_func,\n",
    "    sla=timedelta(minutes=15),  # SLA set here\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. Globally via `default_args`\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'sla': timedelta(minutes=15)  # Applies to all tasks unless overridden\n",
    "}\n",
    "dag = DAG('my_dag', default_args=default_args, schedule_interval='@daily')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ⏱️ About `timedelta`\n",
    "- Imported from: `from datetime import timedelta`\n",
    "- Accepts: `days`, `seconds`, `minutes`, `hours`, `weeks`\n",
    "- Example:\n",
    "  ```python\n",
    "  timedelta(days=4, hours=10, minutes=20, seconds=30)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Reporting & Email Alerts\n",
    "\n",
    "#### 📬 Email Alerts via `default_args`\n",
    "```python\n",
    "default_args = {\n",
    "    'email': ['you@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'email_on_success': False\n",
    "}\n",
    "```\n",
    "\n",
    "#### 📤 Manual Emails with `EmailOperator`\n",
    "Used when you want to send a **custom email** regardless of task outcome:\n",
    "```python\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "email_task = EmailOperator(\n",
    "    task_id='email_task',\n",
    "    to='you@example.com',\n",
    "    subject='Task Complete',\n",
    "    html_content='<p>Your task has completed!</p>',\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "> 📌 Note: Email sending **requires SMTP setup** in `airflow.cfg` (`smtp_host`, `smtp_user`, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 Quick Comparison Table\n",
    "\n",
    "| Feature                     | SLA via Task     | SLA via `default_args` | Email Alerts via `default_args` | Custom Emails via `EmailOperator` |\n",
    "|----------------------------|------------------|-------------------------|----------------------------------|------------------------------------|\n",
    "| Applies to                 | Specific task    | All tasks in DAG        | All tasks (on failure/success)   | Any situation                      |\n",
    "| Granularity                | High             | Broad                   | Broad                            | High                               |\n",
    "| Triggers email on miss     | ✅                | ✅                       | ✅                                | Only if you define it             |\n",
    "| Appears in Web UI          | ✅                | ✅                       | No                               | No                                 |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640f182",
   "metadata": {},
   "source": [
    "Here’s a complete **Airflow DAG example** that includes:\n",
    "\n",
    "- An SLA defined per task ✅  \n",
    "- Email alerts for failures and retries ✅  \n",
    "- A custom email sent using `EmailOperator` ✅  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Full DAG Example: SLA & Email Alerts\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default args\n",
    "default_args = {\n",
    "    'owner': 'diwash',\n",
    "    'start_date': datetime(2025, 4, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=2),\n",
    "    'email': ['your_email@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'email_on_success': False,\n",
    "    'sla': timedelta(minutes=5)  # SLA applied globally\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(\n",
    "    'sla_email_example_dag',\n",
    "    default_args=default_args,\n",
    "    description='Example DAG with SLA and email alerts',\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    tags=['example', 'SLA']\n",
    ") as dag:\n",
    "\n",
    "    # Sample task function\n",
    "    def my_task():\n",
    "        import time\n",
    "        time.sleep(10)  # Simulate processing\n",
    "        print(\"Task is done.\")\n",
    "\n",
    "    # Python task with custom SLA (overriding global SLA)\n",
    "    task1 = PythonOperator(\n",
    "        task_id='run_my_task',\n",
    "        python_callable=my_task,\n",
    "        sla=timedelta(seconds=8)  # SLA specific to this task\n",
    "    )\n",
    "\n",
    "    # Custom email task using EmailOperator\n",
    "    notify = EmailOperator(\n",
    "        task_id='notify_completion',\n",
    "        to='your_email@example.com',\n",
    "        subject='Task Completed!',\n",
    "        html_content='<p>run_my_task has completed successfully.</p>'\n",
    "    )\n",
    "\n",
    "    # Task pipeline\n",
    "    task1 >> notify\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ What You’ll Need to Configure:\n",
    "To make this work, ensure:\n",
    "1. You set your real email in the `email` field and `to=` field of the `EmailOperator`.\n",
    "2. Configure your SMTP in `airflow.cfg`:\n",
    "\n",
    "```ini\n",
    "[smtp]\n",
    "smtp_host = smtp.gmail.com\n",
    "smtp_starttls = True\n",
    "smtp_ssl = False\n",
    "smtp_user = your_email@gmail.com\n",
    "smtp_password = your_password_or_app_password\n",
    "smtp_port = 587\n",
    "smtp_mail_from = your_email@gmail.com\n",
    "```\n",
    "\n",
    "> ⚠️ Use **App Password** if using Gmail with 2FA.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a version of this DAG that logs SLA misses to a custom log or sends a Slack alert instead of email?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21865d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
