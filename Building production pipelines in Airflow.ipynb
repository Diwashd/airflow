{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47687e31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Here’s a breakdown and **comparison** of **templated vs non-templated tasks** in Airflow, with a focus on the BashOperator, as covered in your transcript:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 What Are Airflow Templates?\n",
    "- Templates in Airflow let you **dynamically substitute values at runtime** using [Jinja templating](https://jinja.palletsprojects.com/).\n",
    "- You use double curly braces `{{ }}` for substitution.\n",
    "- This keeps your DAGs **cleaner**, **more flexible**, and **scalable**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛑 Non-Templated Example\n",
    "\n",
    "If you need to log:\n",
    "```\n",
    "Reading file1.txt\n",
    "Reading file2.txt\n",
    "```\n",
    "You’d need **2 separate tasks** manually:\n",
    "\n",
    "```python\n",
    "task1 = BashOperator(\n",
    "    task_id='read_file1',\n",
    "    bash_command='echo Reading file1.txt',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task2 = BashOperator(\n",
    "    task_id='read_file2',\n",
    "    bash_command='echo Reading file2.txt',\n",
    "    dag=dag,\n",
    ")\n",
    "```\n",
    "\n",
    "> ❌ Hard to maintain if you have 100+ files  \n",
    "> ❌ Repetitive code  \n",
    "> ❌ Not flexible\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Templated Example\n",
    "\n",
    "You define a **template string** with a placeholder using Jinja:\n",
    "\n",
    "```python\n",
    "templated_command = 'echo Reading {{ params.filename }}'\n",
    "```\n",
    "\n",
    "Now reuse it:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    'templated_bash_operator',\n",
    "    start_date=datetime(2025, 4, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    task1 = BashOperator(\n",
    "        task_id='read_file1',\n",
    "        bash_command=templated_command,\n",
    "        params={'filename': 'file1.txt'}\n",
    "    )\n",
    "\n",
    "    task2 = BashOperator(\n",
    "        task_id='read_file2',\n",
    "        bash_command=templated_command,\n",
    "        params={'filename': 'file2.txt'}\n",
    "    )\n",
    "\n",
    "    task1 >> task2\n",
    "```\n",
    "\n",
    "> ✅ Cleaner, reusable logic  \n",
    "> ✅ Only the `params` need to change  \n",
    "> ✅ Easy to loop/generate dynamically if needed\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Bonus: Looping Over Files (Dynamic Task Mapping in Airflow 2.3+)\n",
    "\n",
    "```python\n",
    "files = ['file1.txt', 'file2.txt', 'file3.txt']\n",
    "\n",
    "from airflow.decorators import task, dag\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "@dag(start_date=datetime(2025, 4, 1), schedule_interval=None, catchup=False)\n",
    "def dynamic_bash_template():\n",
    "\n",
    "    @task\n",
    "    def list_files():\n",
    "        return files\n",
    "\n",
    "    def make_task(filename):\n",
    "        return BashOperator(\n",
    "            task_id=f'read_{filename.replace(\".\", \"_\")}',\n",
    "            bash_command='echo Reading {{ params.filename }}',\n",
    "            params={'filename': filename}\n",
    "        )\n",
    "\n",
    "    file_list = list_files()\n",
    "    file_list.output.map(make_task)\n",
    "\n",
    "dag = dynamic_bash_template()\n",
    "```\n",
    "\n",
    "> ✅ Fully dynamic with Airflow 2.3+ task mapping!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba44f6f",
   "metadata": {},
   "source": [
    "### **Creating a templated BashOperator**\n",
    "You've successfully created a BashOperator that cleans a given data file by executing a script called cleandata.sh. This works, but unfortunately requires the script to be run only for the current day. Some of your data sources are occasionally behind by a couple of days and need to be run manually.\n",
    "\n",
    "You successfully modify the cleandata.sh script to take one argument - the date in YYYYMMDD format. Your testing works at the command-line, but you now need to implement this into your Airflow DAG. For now, use the term {{ ds_nodash }} in your template - you'll see exactly what this means later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021591c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring'\n",
    "templated_command = \"\"\"\n",
    "bash cleandata.sh {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          dag=cleandata_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2208ef",
   "metadata": {},
   "source": [
    "### **Templates with multiple arguments**\n",
    "You wish to build upon your previous DAG and modify the code to support two arguments - the date in YYYYMMDD format, and a file name passed to the cleandata.sh script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'supportdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fc052",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🐍 1. **Templated `PythonOperator` Example**\n",
    "\n",
    "The `PythonOperator` runs Python functions and can access context variables like `ds`, `ts`, etc., which can be templated.\n",
    "\n",
    "#### ✅ Example: Use `ds` (execution date) in a Python function\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def print_execution_date(execution_date, **kwargs):\n",
    "    print(f\"Running DAG on: {execution_date}\")\n",
    "\n",
    "with DAG(\n",
    "    dag_id='templated_python_operator',\n",
    "    start_date=datetime(2025, 4, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    task = PythonOperator(\n",
    "        task_id='print_execution_date',\n",
    "        python_callable=print_execution_date,\n",
    "        op_args=['{{ ds }}'],  # Jinja templating in op_args\n",
    "        provide_context=True\n",
    "    )\n",
    "```\n",
    "\n",
    "> 🧠 `{{ ds }}` is replaced by the current DAG execution date (e.g. `2025-04-11`)\n",
    "\n",
    "---\n",
    "\n",
    "### 📧 2. **Templated `EmailOperator` Example**\n",
    "\n",
    "The `EmailOperator` sends emails using Airflow’s configured email backend. You can use templating to customize the subject or body.\n",
    "\n",
    "#### ✅ Example: Use templated subject and HTML body\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id='templated_email_operator',\n",
    "    start_date=datetime(2025, 4, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    send_email = EmailOperator(\n",
    "        task_id='send_email_task',\n",
    "        to='team@example.com',\n",
    "        subject='Report for {{ ds }}',\n",
    "        html_content=\"\"\"\n",
    "            <h3>Daily Report</h3>\n",
    "            <p>The DAG ran on {{ ds }} and completed successfully.</p>\n",
    "        \"\"\",\n",
    "    )\n",
    "```\n",
    "\n",
    "> 📨 Output will be:\n",
    "> - **Subject:** Report for 2025-04-11\n",
    "> - **Body:** HTML showing that the DAG ran successfully on that day\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Summary\n",
    "\n",
    "| Operator        | What’s Templated?                     | Useful Context Vars     | Notes |\n",
    "|----------------|----------------------------------------|--------------------------|-------|\n",
    "| `BashOperator`  | Shell command (`bash_command`)         | `{{ ds }}`, `{{ params }}` | Clean logging, flexible tasks |\n",
    "| `PythonOperator`| Function args (`op_args`, `op_kwargs`) | `{{ ds }}`, `{{ ts }}`, `{{ dag }}` | Use `provide_context=True` |\n",
    "| `EmailOperator` | `subject`, `html_content`, `body`     | `{{ ds }}`, `{{ task_instance }}` | Useful for alerts and reports |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362ca1b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🔁 1. **Advanced Templating with `for` Loops in Jinja**\n",
    "\n",
    "#### ✅ Use Case:\n",
    "Instead of creating multiple tasks for multiple filenames, iterate through a list in a single task using Jinja's `for` loop.\n",
    "\n",
    "#### 🧪 Example:\n",
    "\n",
    "```python\n",
    "templated_command = \"\"\"\n",
    "{% for filename in params.filenames %}\n",
    "    echo Reading {{ filename }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "read_files = BashOperator(\n",
    "    task_id='read_multiple_files',\n",
    "    bash_command=templated_command,\n",
    "    params={'filenames': ['file1.txt', 'file2.txt']},\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "> 🧠 This creates a single task that logs:\n",
    "> ```\n",
    "> Reading file1.txt\n",
    "> Reading file2.txt\n",
    "> ```\n",
    "\n",
    "> ⚡ **Scales well** for 100+ files without needing 100 tasks!\n",
    "\n",
    "---\n",
    "\n",
    "### 📅 2. **Built-in Template Variables**\n",
    "\n",
    "Airflow injects special runtime context variables into templates. These include:\n",
    "\n",
    "| Variable          | Description                                           | Example Output     |\n",
    "|-------------------|-------------------------------------------------------|---------------------|\n",
    "| `{{ ds }}`         | DAG run date (YYYY-MM-DD)                            | `2025-04-11`        |\n",
    "| `{{ ds_nodash }}`  | Same as `ds` but without dashes                      | `20250411`          |\n",
    "| `{{ prev_ds }}`    | Previous DAG run date                                | `2025-04-10`        |\n",
    "| `{{ prev_ds_nodash }}` | Previous DAG run date without dashes            | `20250410`          |\n",
    "| `{{ dag }}`        | Current DAG object                                   | `<DAG object>`      |\n",
    "| `{{ conf }}`       | Airflow config values (if needed)                   | `<config>`          |\n",
    "\n",
    "> 🔍 These are **strings**, not datetime objects.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 3. **Airflow Macros**\n",
    "\n",
    "The `macros` variable allows access to helper functions and Python standard objects inside templates.\n",
    "\n",
    "#### 📦 Key Macros:\n",
    "\n",
    "| Macro                     | Description                                                | Example                             |\n",
    "|--------------------------|------------------------------------------------------------|-------------------------------------|\n",
    "| `macros.datetime`         | Python's `datetime.datetime` object                        | Use for date formatting             |\n",
    "| `macros.timedelta`        | Python's `timedelta` object                                | Useful for date math                |\n",
    "| `macros.uuid`             | Generates UUIDs                                            | `1b4e28ba-2fa1-11d2-883f-0016d3cca427` |\n",
    "| `macros.ds_add(ds, n)`    | Adds or subtracts days from a date string                  | `macros.ds_add('20200418', 2)` → `20200420` |\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Reference\n",
    "🔗 Official Airflow Macros Documentation:  \n",
    "👉 [https://airflow.apache.org/docs/stable/macros-ref.html](https://airflow.apache.org/docs/stable/macros-ref.html)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Summary\n",
    "\n",
    "| Feature            | Benefit                                      | Use Case                              |\n",
    "|--------------------|-----------------------------------------------|----------------------------------------|\n",
    "| Jinja `for` loop   | Iterates over lists                          | Single task to handle multiple items   |\n",
    "| Template variables | Access to DAG run context                    | Dynamic file naming, logs, etc.        |\n",
    "| Macros             | Extend templating with Python-like logic     | Date math, UUID generation, etc.       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e5480",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🌍 Real-World Scenario:\n",
    "> You’re building a DAG that processes daily log files for multiple departments. Each log file is named like `sales_{{ ds }}.log`, `marketing_{{ ds }}.log`, etc. You want to:\n",
    "- Dynamically generate file paths using `ds`\n",
    "- Iterate through departments\n",
    "- Log the processing\n",
    "- Use macros to calculate yesterday’s date and include it in the log\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ DAG with Full Templating Power\n",
    "\n",
    "### 📦 Requirements:\n",
    "- Process files for multiple departments in a single BashOperator\n",
    "- Use `{{ ds }}` for the current run date\n",
    "- Use `macros.ds_add(ds, -1)` for yesterday’s date\n",
    "- Iterate using a Jinja `for` loop\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Code Example\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2025, 4, 10),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'templated_log_processor',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    departments = ['sales', 'marketing', 'finance']\n",
    "\n",
    "    templated_command = \"\"\"\n",
    "    echo \"Run date: {{ ds }}\"\n",
    "    echo \"Yesterday was: {{ macros.ds_add(ds, -1) }}\"\n",
    "    \n",
    "    {% for dept in params.departments %}\n",
    "        echo \"Processing {{ dept }} logs for date {{ ds }}...\"\n",
    "        echo \"File: /logs/{{ dept }}/{{ ds }}.log\"\n",
    "        echo \"Archived version: /archive/{{ dept }}/{{ macros.ds_add(ds, -1) }}.log\"\n",
    "    {% endfor %}\n",
    "    \"\"\"\n",
    "\n",
    "    process_logs = BashOperator(\n",
    "        task_id='process_daily_logs',\n",
    "        bash_command=templated_command,\n",
    "        params={'departments': departments},\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 What It Does\n",
    "\n",
    "✅ Logs the current DAG run date  \n",
    "✅ Logs yesterday's date using a macro  \n",
    "✅ Iterates through each department  \n",
    "✅ Prints both current and archived log file paths\n",
    "\n",
    "---\n",
    "\n",
    "### 📤 Output (for 2025-04-11 DAG run):\n",
    "\n",
    "```\n",
    "Run date: 2025-04-11\n",
    "Yesterday was: 2025-04-10\n",
    "\n",
    "Processing sales logs for date 2025-04-11...\n",
    "File: /logs/sales/2025-04-11.log\n",
    "Archived version: /archive/sales/2025-04-10.log\n",
    "\n",
    "Processing marketing logs for date 2025-04-11...\n",
    "File: /logs/marketing/2025-04-11.log\n",
    "Archived version: /archive/marketing/2025-04-10.log\n",
    "\n",
    "Processing finance logs for date 2025-04-11...\n",
    "File: /logs/finance/2025-04-11.log\n",
    "Archived version: /archive/finance/2025-04-10.log\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Learnings Recap:\n",
    "| Feature Used       | Why It’s Useful                                      |\n",
    "|--------------------|------------------------------------------------------|\n",
    "| `{{ ds }}`         | Gets the DAG run date dynamically                    |\n",
    "| `macros.ds_add()`  | Allows date math inside templates                    |\n",
    "| Jinja `for` loop   | Avoids writing one task per department               |\n",
    "| `params` dict      | Passes custom variables into templated commands      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7379f55",
   "metadata": {},
   "source": [
    "### **Using lists with templates**\n",
    "Once again, you decide to make some modifications to the design of your cleandata workflow. This time, you realize that you need to run the command cleandata.sh with the date argument and the file argument as before, except now you have a list of 30 files. You do not want to create 30 tasks, so your job is to modify the code to support running the argument for 30 or more files.\n",
    "\n",
    "The Python list of files is already created for you, simply called filelist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n",
    "                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a058c73",
   "metadata": {},
   "source": [
    "### **Sending templated emails**\n",
    "While reading through the Airflow documentation, you realize that various operations can use templated fields to provide added flexibility. You come across the docs for the EmailOperator and see that the content can be set to a template. You want to make use of this functionality to provide more detailed information regarding the output of a DAG run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1680eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2023, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febc408",
   "metadata": {},
   "source": [
    " **Real-world DAG using branching** \n",
    "---\n",
    "\n",
    "## 🌍 Real-World Scenario:  \n",
    "> You manage data quality checks. You want to run **different validation tasks on odd and even days**.  \n",
    "For example:\n",
    "- **Even days**: Run full data integrity check.\n",
    "- **Odd days**: Run only basic null check.\n",
    "\n",
    "You’ll use `BranchPythonOperator` to decide which path to follow.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Key Concepts Used:\n",
    "| Feature              | Role                                                                 |\n",
    "|----------------------|----------------------------------------------------------------------|\n",
    "| `BranchPythonOperator` | Controls the flow based on logic (even/odd days)                     |\n",
    "| `kwargs['ds_nodash']`  | Provides runtime date in `YYYYMMDD` format                          |\n",
    "| `.provide_context=True`| Makes runtime context available inside your Python function         |\n",
    "| `.>>` & `<<` (bitshift)| Sets task dependencies in a readable way                            |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Code Example:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'start_date': datetime(2025, 4, 10),\n",
    "}\n",
    "\n",
    "def choose_path(**kwargs):\n",
    "    run_date = int(kwargs['ds_nodash'])\n",
    "    if run_date % 2 == 0:\n",
    "        return 'full_integrity_check'\n",
    "    else:\n",
    "        return 'null_check'\n",
    "\n",
    "with DAG(\n",
    "    'branching_data_quality_dag',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    start = EmptyOperator(task_id='start')\n",
    "\n",
    "    branch = BranchPythonOperator(\n",
    "        task_id='branch_task',\n",
    "        python_callable=choose_path,\n",
    "        provide_context=True\n",
    "    )\n",
    "\n",
    "    full_integrity_check = EmptyOperator(task_id='full_integrity_check')\n",
    "    full_check_followup = EmptyOperator(task_id='full_check_followup')\n",
    "\n",
    "    null_check = EmptyOperator(task_id='null_check')\n",
    "    null_check_followup = EmptyOperator(task_id='null_check_followup')\n",
    "\n",
    "    join = EmptyOperator(task_id='join', trigger_rule='none_failed_or_skipped')\n",
    "\n",
    "    # Set dependencies\n",
    "    start >> branch\n",
    "    branch >> full_integrity_check >> full_check_followup >> join\n",
    "    branch >> null_check >> null_check_followup >> join\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📤 What It Does:\n",
    "\n",
    "- Runs `branch_task` after the `start` task\n",
    "- If the day is **even** (e.g., 20250412), it:\n",
    "  - Runs `full_integrity_check` → `full_check_followup`\n",
    "- If the day is **odd** (e.g., 20250411), it:\n",
    "  - Runs `null_check` → `null_check_followup`\n",
    "- Both paths merge into the `join` task\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Airflow Graph View:\n",
    "\n",
    "```plaintext\n",
    "start\n",
    "  |\n",
    "branch_task\n",
    " /         \\\n",
    "full       null\n",
    " |           |\n",
    "followup   followup\n",
    "   \\       /\n",
    "     join\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Optional Enhancements:\n",
    "\n",
    "- Replace `EmptyOperator` with real logic (e.g., `PythonOperator`, `BashOperator`)\n",
    "- Add logging inside each task to indicate which path is being followed\n",
    "- Use `trigger_rule='none_failed_or_skipped'` on `join` to handle skipped paths gracefully\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37036c",
   "metadata": {},
   "source": [
    "### **Define a BranchPythonOperator**\n",
    "After learning about the power of conditional logic within Airflow, you wish to test out the BranchPythonOperator. You'd like to run a different code path if the current execution date represents a new year (ie, 2020 vs 2019).\n",
    "\n",
    "The DAG is defined for you, along with the tasks in question. Your current task is to implement the BranchPythonOperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_task >> current_year_task\n",
    "branch_task >> new_year_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a117ff",
   "metadata": {},
   "source": [
    "### **Branch troubleshooting**\n",
    "While working with a workflow defined by a colleague, you notice that a branching operator executes, but there's never any change in the DAG results. You realize that regardless of the state defined by the branching operator, all other tasks complete, even as some should be skipped.\n",
    "\n",
    "Use what you've learned to determine the most likely reason that the branching operator is ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e814bb",
   "metadata": {},
   "source": [
    "Great observation — this kind of issue is actually pretty common when using the `BranchPythonOperator` for the first time. Here's a **quick diagnosis and the most likely cause**:\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **Root Cause: Downstream Tasks Aren’t Properly Connected to the Branch**\n",
    "> **All tasks run** despite the branch decision because **they are not directly downstream of the BranchPythonOperator.**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Branching Works **Only If:**\n",
    "1. All downstream tasks from the branch are **direct children** of the `BranchPythonOperator`.\n",
    "2. The non-selected tasks will only be skipped **if they are part of the branch’s downstream**.\n",
    "3. Any task that is not explicitly part of the branching path **will still execute** as usual.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Visualizing the Wrong Setup:\n",
    "\n",
    "```plaintext\n",
    "start\n",
    "  |\n",
    "branch_task\n",
    "  |\n",
    "task_a     ← not directly downstream\n",
    "task_b     ← not directly downstream\n",
    "```\n",
    "\n",
    "> Even if `branch_task` returns only `task_a`, `task_b` still runs because it's not controlled by the branch.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Correct Setup (All Paths Flow Through Branch):\n",
    "\n",
    "```plaintext\n",
    "start\n",
    "  |\n",
    "branch_task\n",
    " /       \\\n",
    "task_a   task_b\n",
    "```\n",
    "\n",
    "> Now, if `branch_task` returns `'task_a'`, only `task_a` runs. `task_b` will be **skipped automatically.**\n",
    "\n",
    "---\n",
    "\n",
    "## ✔ Fix the Issue by:\n",
    "- Making sure **every downstream path from the branch is explicitly connected**\n",
    "- Using Airflow’s `.set_downstream()` or `>>` operators correctly\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Example Fix in Code:\n",
    "\n",
    "```python\n",
    "branch >> task_a\n",
    "branch >> task_b\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00207e4",
   "metadata": {},
   "source": [
    "### **Production pipeline in Airflow**:\n",
    "\n",
    "| 🔢 | Topic                    | Command / Example                             | 🔍Explanation                                                                 |\n",
    "|-----|---------------------------|------------------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| 1️⃣ | **Run a Task**           | `airflow tasks test <dag_id> <task_id> <date>` | Runs **one task** from a DAG for a specific date (for testing).                     |\n",
    "| 2️⃣ | **Run a Full DAG**       | `airflow dags trigger -e <date> <dag_id>`     | Runs the **entire DAG** as if it were executed on that date.                        |\n",
    "| 3️⃣ | **BashOperator**         | `bash_command=\"echo Hello\"`                   | Runs Bash commands inside your DAG.                                                 |\n",
    "| 4️⃣ | **PythonOperator**       | `python_callable=my_function`                | Runs a Python function.                                                             |\n",
    "| 5️⃣ | **BranchPythonOperator**| Like PythonOperator + `provide_context=True`  | Chooses **which task to run next** based on logic (e.g., even vs odd day).          |\n",
    "| 6️⃣ | **FileSensor**           | `filepath='/data/file.csv'`                  | Waits until a file exists at the given location before continuing.                  |\n",
    "| 7️⃣ | **Template Fields**      | Example: `bash_command=\"{{ ds }}\"`            | Use **Jinja templates** to insert dynamic values like execution date.               |\n",
    "| 8️⃣ | **Check Template Support**| `help(BashOperator)` in Python                | Shows which fields (like `bash_command`) can use templates (look for `template_fields`). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb536bfe",
   "metadata": {},
   "source": [
    "### **Creating a production pipeline #1**\n",
    "You've learned a lot about how Airflow works - now it's time to implement your workflow into a production pipeline consisting of many objects including sensors and operators. Your boss is interested in seeing this workflow become automated and able to provide SLA reporting as it provides some extra leverage for closing a deal the sales staff is working on. The sales prospect has indicated that once they see updates in an automated fashion, they're willing to sign-up for the indicated data service.\n",
    "\n",
    "From what you've learned about the process, you know that there is sales data that will be uploaded to the system. Once the data is uploaded, a new file should be created to kick off the full processing, but something isn't working correctly.\n",
    "\n",
    "Refer to the source code of the DAG to determine if anything extra needs to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ccd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "# Import the needed operators\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import date, datetime\n",
    "\n",
    "def process_data(**context):\n",
    "  file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
    "  file.write(f'Data processed on {date.today()}')\n",
    "  file.close()\n",
    "\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2023,4,1)})\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=5,\n",
    "                    timeout=15,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660a77c",
   "metadata": {},
   "source": [
    "### **Creating a production pipeline #2**\n",
    "Continuing on your last workflow, you'd like to add some additional functionality, specifically adding some SLAs to the code and modifying the sensor components.\n",
    "\n",
    "Refer to the source code of the DAG to determine if anything extra needs to be added. The default_args dictionary has been defined for you, though it may require further modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f69ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6561eb8a",
   "metadata": {},
   "source": [
    "### **Adding the final changes to your pipeline**\n",
    "To finish up your workflow, your manager asks that you add a conditional logic check to send a sales report via email, only if the day is a weekday. Otherwise, no email should be sent. In addition, the email task should be templated to include the date and a project name in the content.\n",
    "\n",
    "The branch callable is already defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "no_email_task = EmptyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,\n",
    "                                   dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
